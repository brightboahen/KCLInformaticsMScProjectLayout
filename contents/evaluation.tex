\section{Testing and Evaluation}
\subsection{Overview}
The previous chapter guided as through the implementation of the design of our system and also presented the framework on which our system was implemented. In this chapter we will look at the methods used to evaluate our system. We will discuss how well our system addressed the issue it was set out to address and how it performed in the tests used in the evaluation.
\subsection{Methodology}
This section presents the methods used in testing the functionalities and features of our system and how well our system performed under each method.
\subsubsection{Browser Tests}
A web browser \footnote{A software to display HTML and interpret JavaScript} was used in evaluating our system as our system operates in a browser setting. There are several web browsers and our system needs to behave in a consistent manner across all the browser platforms. Our system uses some experimental features that are not implemented on all browsers yet, due to this the browser test was predominantly performed on google chrome. \cite{website:GoogleChrome}
The figure \ref{fig:homeScreen} below shows how google chrome rendered our system's UI.
\begin{figure}[!ht]
\caption{Google Chrome : Rendering of our system's home Screen}
    \label{fig:homeScreen}
    \centering
    \includegraphics[scale=0.3]{figures/home_screen}
\end{figure}
Figure \ref{fig:homeScreen} above is annotated the UI components that were expected to be on diplay on the home screen. These are :
\begin{itemize}
    \item menu button -  on the top left corner,
    \item toolbar - on the top right corner,
    \item menu tabs - below the menu button, 
    \item Add new button - bottom right corner
\end{itemize}
Figure \ref{fig:canvas} below illustrates the result of our system's classroom component in a web browser \cite{website:GoogleChrome}.
\begin{figure}[!ht]
    \caption{Google Chrome : Rendering of the classroom}
    \label{fig:canvas}
    \centering
    \includegraphics[scale=0.3]{figures/classroom}
\end{figure}
In figure \ref{fig:canvas} above we can see the \emph{History Manager} rendered clearly on the right side with some previous seating arrangements. To the left of the \emph{History Manager} is the \emph{classroom canvas}. The result is as expected as the rule has been applied and optimal pairings coloured ``green'' and worst pairings or groups coloured ``red''.

\subsubsection{Unit Tests}
Unit testing is the process of testing individual units(components) or groups of related units. \cite{runeson2006survey}. Our system is made of several units that together compose the system. It was necessary to test these units to ensure they perform their responsibilities effectively and efficiently.

We use Web Component Tester(WCT) \cite{website:Polymer-Github} to test our components. WCT provides a browser based environment for us to test our components.
\begin{table}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{Unit Tests on Google Chrome} \\
        \hline
        Unit(component)         &     Test Result \\
        \hline
        Classroom canvas        &     Passed \\
        HistoryStack            &     Passed \\
        MenuTab                 &     Passed \\
        Toolbar                 &     Passed \\
        \hline
    \end{tabular}
    \caption{\label{tab:unit-chrome}Unit tests performed on core components of our system in google chrome browser.}
\end{table}

\begin{table}
    \begin{tabular}{|c|c|}
        \hline
        \multicolumn{2}{|c|}{Unit Tests on Internet Explorer} \\
        \hline
        Unit(component)         &     Test Result \\
        \hline
        Classroom canvas        &     Passed \\
        HistoryStack            &     Failed \\
        MenuTab                 &     Passed \\
        Toolbar                 &     Failed \\
        \hline
    \end{tabular}
    \caption{\label{tab:unit-explorer}Unit tests performed on core components of our system on internet explorer browser.}
\end{table}
As we can see from table \ref{tab:unit-chrome} the critical components passed on the unit tests but on Internet Explorer (alternate web browser) two of the components failed due to some experimental web features that are not fully supported by Internet Explorer.
\subsubsection{User Tests} \label{sub:user-testing}
In \ref{sub:userRequirments} section we discussed the features and functionalities expected of our system by the user. With this in mind we conducted a ``black box'' testing with users by asking potential serious \footnote{ serious user refers to a teacher} users who have no prior knowledge of the system to use the system and we evaluated their response on the clarity of use and whether they used less cognitive skills when performing their seating tasks.
\begin{table}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multicolumn{5}{|c|}{User Testing} \\
        \hline
        Age & Sex & Subject & Clarity & Level of cognition \\
        \hline
        30  & Female & Drama & 3 out of 5 & 4 out 5\\
        \hline
    \end{tabular}
    \caption{\label{tab:user-testing1} Black box testing with users - first round}
\end{table}


Table \ref{tab:user-testing1} illustrates the response of the user after her first encounter with our system.
\begin{table}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multicolumn{5}{|c|}{User Testing} \\
        \hline
        Age & Sex & Subject & Clarity & Level of cognition \\
        \hline
        30  & Female & Drama & 4 out of 5 & 2 out 5\\
        \hline
    \end{tabular}
    \caption{\label{tab:user-testing2} Black box testing with users - second round}
\end{table}


Table \ref{tab:user-testing2} illustrates the response of the user after her second encounter with our system.

From tables \ref{tab:user-testing1} and \ref{tab:user-testing2} we can see that the users perception of our system improved, that is she found it more clear the second time although the level of clarity was average the first time and the level of cognition( that is the thinking she had to do) also reduced dramatically on her second attempt. 
\subsubsection{Remarks on tests}
We based based the user tests in  \ref{sub:user-testing} on two main concepts.
\begin{itemize}
    \item Clarity,
    \item Cognitive Skills.
\end{itemize}
This is because we can use these two as a metric to judge the system whether it meets all the user requirements mentioned in \ref{sub:userRequirments} . Clarity involves all the steps of the use while performing their tasks in the system and cognitive skills involves how hard they have had to think about their tasks. A high cognitive value would indicate the system is not achieving its adaptivity requirement and if they steps are not clear enough the user cannot perform their tasks. The system fails its overall aim.

The result from the tests in \ref{tab:user-testing1} and \ref{tab:user-testing2} cannot be conclusive as we were only able to get one serious user to undertake the test. It may not be conclusive but it gave us an indication of the potential of the system in achieving its goals. We also could not conduct real time test by asking teachers to use the system for a class and report on how well the seatings recommended by the system performed.

\subsection{Evaluation}
This section assesses the results of our system against the mandatory requirements we presented in chapter \ref{sec:designRequirements} of this paper. In table \ref{tab:requirements-evaluation} we look at the status of each requirement.
\subsubsection{Requirements}
\begin{longtable}{|L|c|L|}\hline
Requirement & Status & Comment \\\hline
Account Registration & Complete &  \multicolumn{1}{m{7cm}|}{The user can create or register an account with our system, by selecting the create account button on the user interface.}\\\hline
Login & Complete  &  \multicolumn{1}{m{7cm}|}{ Our system provides a registered user the functionality to access system features}\\\hline
Logout & Complete  &  \multicolumn{1}{m{7cm}|}{ Our system provides the user with logout functionality to safely leave the system}\\\hline
Add class groups & Complete  &  \multicolumn{1}{m{7cm}|}{The user is able add a new class group to his or her workspace}\\\hline 
Create seating arrangements & Complete  &  \multicolumn{1}{m{7cm}|}{The user is able create seating arrangements by dragging and dropping models of students on the classroom canvas.}\\\hline
Upload pupil data & Complete  &  \multicolumn{1}{m{7cm}|}{The user is able to upload data in CSV format.}\\\hline
Save seating plans & Complete &  \multicolumn{1}{m{7cm}|}{The user is able to save new seating plans through the history manager}\\\hline
Retrieve seating plans & Complete &  \multicolumn{1}{m{7cm}|}{The user is able to retrieve old seating arrangements through the history manager }\\\hline
Update seating plans & Complete &  \multicolumn{1}{m{7cm}|}{The user is able to update existing seating arrangements through the history manager}\\\hline
Scoring System & Complete &  \multicolumn{1}{m{7cm}|}{The user is able to evaluate a seating arrangement or plan through the history item score mechanism}\\\hline
Seating Recommendation & Complete  &  \multicolumn{1}{m{7cm}|}{Our system is able to recommend to the user, based on the rules implicitly inferred from saved seating arrangements with highest score}\\\hline
User Monitoring & Complete &  \multicolumn{1}{m{7cm}|}{The system is able to track the user's system usage by recording times the log in and out of the system, the features they frequently use or spend most time. }\\\hline
\caption{\label{tab:requirements-evaluation}Table of requirements and their evaluation}
\end{longtable}
\subsubsection{Design}
Our system has been designed to be transparent and an enjoyable experience for the user. Our evaluation of the system with regards to transparency and enjoyable experience as a result of user testing \ref{sub:user-testing} is inconclusive. The only user used in testing gave a general feedback that showed our system met its transparency and enjoyable design but that is the view of one user.
\subsubsection{Similar Systems}
In the literature review chapter of this paper we discussed existing systems. In this sub section we will evaluate our system's result against some of these systems.

Our system is able to recommend optimal seating arrangements, pairings using rules that have not been used in any of the existing systems. The system is also able to monitor the users activities, a feature missing in the other system due to their rigid nature. This is supported by the JSON(JavaScript Object Notation) extracted from the system's database in listing \ref{lst:modelRep}.
\begin{lstlisting}[caption={User Tracking Model Representation}, label={lst:modelRep}]
      "myRep" : {
    "CE33hObsZDU4E6dyL3KLcauCK7a2" : {
      "myModelRep" : {
        "ExperienceWithSystem" : {
          "addClass" : {
            "avTime" : 6,
            "initialTime" : 7
          },
          "csvUpload" : {
            "avTime" : 4,
            "initialTime" : 5
          },
          "logIn" : {
            "timeTaken" : 30
          },
          "profile" : {
            "timeTaken" : 8
          },
          "reg" : {
            "timeTaken" : 27
          },
          "settings" : {
            "avTime" : 3,
            "initialTime" : 2
          }
        },
        "adapt" : {
          "my-floating-button" : {
            "exp" : 13,
            "sys" : {
              "seconds" : 0
            }
          }
        },
        "personal" : {
          "name" : "Lauren Main",
          "sex" : "female",
          "subject" : "Drama"
        },
        "preferences" : {
          "my-menu-toggle" : {
            "count" : 10
          }
        },
        "sysUse" : {
          "seconds" : 45
        }
      }
    }
\end{lstlisting}


\subsection{Summary}
In this chapter we have presented evidence of how well our system achieves its aims and objectives. We have seen how the individual components performed and how the overall system meets the requirement of a user.